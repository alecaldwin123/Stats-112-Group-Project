---
title: "Group Project Stats 112"
author: "Alec, Jay, Lynn"
date: "5/7/2021"
output: html_document
---

##Setup ---

```{r message = FALSE, warning = FALSE}
library(tidyverse)
library(nlme)
library(mgcv)
aids = read.csv("aids.csv")

glimpse(aids)
```

##Exploratory Data Analysis --- 

```{r}
subjects = unique(aids$id)
subjects = max(subjects)
```

The number of subjects in the dataset is `r subjects`
The number of the covariates is `r ncol(aids) `

##Univariate summaries 

```{r}
summary(aids)

aids %>% 
  summarize(mean_log_cd4 = mean(log_cd4, na.rm = TRUE), 
            stdev_log_cd4 = sd(log_cd4, na.rm = TRUE))
aids %>% 
  group_by(treatment) %>% 
  summarize(mean_log_cd4 = mean(log_cd4, na.rm = TRUE), 
            stdev_log_cd4 = sd(log_cd4, na.rm = TRUE))
```
We can see that for this longitudinal study, the log_cd4 values range from 0 to 6.29, with a median of 2.9 and a mean of 2.87. The 3rd quantile is around 3.57, but the maximum is 6.297, which may indicate that we have potential outlier. When we look at the overall mean and standard deviation of log(cd4), the mean is around 2.87 and the standard deviation is around 1.07. We can see that the maximum log_cd4 value 6.29 is more than 2 standard deviations away from the overall mean, supporting our estimate that that value could be an outlier in the dataset. We can also see in the upcoming boxplots that there are some outliers.
When we look at the mean and standard deviations of log_cd4 for each treatment group, we can see that as the treatment code increases, the mean and standard deviation of log_cd4 also increases.



```{r}
# Distribution of Age
aids %>% 
  ggplot(aes(x = age)) + 
  geom_histogram(bins = 15, color = "black", fill = "gray")
```


```{r}
# Distribution of people in each treatment group
table(aids$treatment)
round(prop.table(table(aids$treatment)), 2)
```



##Bivariate summaries 
```{r}
#Boxplot based on treatment group
aids %>% 
  ggplot(aes(x = factor(treatment), y = log_cd4, 
             fill = factor(treatment))) +
  geom_boxplot()
```

```{r}
#Boxplot based on gender
aids %>% 
  ggplot(aes(x = gender, y = log_cd4, fill = gender)) +
  geom_boxplot()
```


##Overall trends of response ---

```{r}
# Mean log_cd4 per week of each treatment 

aids %>% 
 ggplot(aes(x = week, y = log_cd4,  color =  factor(treatment))) +  
 labs(title = "Smoothed Trend of Each Treatment",
      y = "Log cd4", x = "Week") +
 geom_smooth(se = FALSE) +
  facet_wrap(~treatment)

```

```{r}

# Trends pf each subject per week separated by treatment
aids %>% 
 ggplot(aes(x = week, y = log_cd4, color = factor(treatment)))+
 geom_point(alpha = 0.3) +
 geom_smooth(se = FALSE) +
 coord_cartesian(ylim = c(2.2,3.3)) + 
 labs(title = "Smoothed Trend of Each Subject",
      y = " Log cd4", x = "Week")
```

```{r}
aids %>% 
  group_by(treatment, week) %>% 
  summarize(mean_log_cd4 = mean(log_cd4, na.rm = TRUE)) %>% 
  ggplot(aes(x = week, y = mean_log_cd4, color = factor(treatment))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", se = FALSE) + 
  labs(x = "Week", y = "Log(CD4)")
```

In this plot, we can see that as the weeks increase, the mean values of log_cd4 for each treatment group stays relatively constant. There does seem to be a large scatter of points that might significantly deviate from the mean.



##Imbalances in the data ---

```{r}
aids %>%
  group_by(id) %>% 
  count() %>% 
  summary()
```

We can see that the number of observations per id ranges from 1 to 9, with a median of 4 and a mean of around 3.8. It is safe to assume that the data is imbalanced, because each subject does not have a constant number of observations.

```{r}
aids %>%
  group_by(treatment) %>% 
  distinct(id) %>% 
  count()
```
We can see that the amount of people in each treatment group is pretty balanced, with each treatment group having around 325 subjects.

#Mistimmed events?
```{r}
aids %>% 
  filter(treatment == 2) %>% 
  ggplot(aes(x = week)) +
  geom_histogram(color = "black", fill = "gray") +
  labs(title = "Week of Treatment 2 group")
```
As we can see from this graph that all of those in treatment two have different timed occasions, if there were a set occasion then it would stack up similar to the base line or at week 0. 

## Outliers ---

We can see from the boxplots above there are quite a few outliers in the log_cd4 per treatment. To see the overall number of outliers we can create a boxplot of the log_cd4 and count the numbers outside of the interquartile range


```{r}
aids %>% 
  ggplot(aes(y = log_cd4)) +
  geom_boxplot()
```
These are the outliers and the total number of outliers are 
```{r}
#outliers values
tibble(outliers = boxplot.stats(aids$log_cd4)$out )

#number of outliers 
observerd = tibble(r = boxplot.stats(aids$log_cd4)$out) %>% 
  nrow()

#expected number of outliers are
expected = nrow(aids) * .05
```
There is about `r observerd` outliers and the expected is `r expected`.
We are below the number of expected number of outliers so the observed outliers are normal.


<p>Let us check the distribution of the log_cd4 and assess its normality</p>
```{r}

aids %>% 
  ggplot(aes(x = log_cd4)) +
   geom_histogram(bins = 15, color = "black", fill = "gray") +
   labs(title = "Histogram of Log_cd4")
  

```

The histogram its approximately normal, but we can get more information but standardizing the log_cd4

```{r}

# Standardized of log_cd4 with mean 0 and sd 1 
log_cd4 = aids$log_cd4

scaled = scale(log_cd4)
colMeans(scaled)
apply(scaled, 2, sd)

tibble(r = scaled) %>% 
  ggplot(aes(x = r)) +
  geom_histogram( color = "black", fill = "gray") +
  labs(title = "Standardized Histogram of Log_cd4")
```

We can see that there is quite a few number of instances after the -2. This indicates heavy tails as well as outliers



##modeling -----
 
```{r}


aids = aids %>% 
  mutate(occasion = ceiling(week),
         gender = factor(gender, level = c("male", "female")),
         treatment = as.factor(treatment))


model1 = lme(log_cd4 ~ occasion + I(occasion^2) + 
               treatment + treatment:occasion + age + gender, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "REML")

summary(model1)
```
Because it is a randomized experiment we do not need the group effect of treatment and therefore treatmetn could be removed in the model



```{r}
model1 = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + age + gender, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "REML")

summary(model1)
```

```{r}
model_no_gender = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + treatment:I(occasion^2) +
               age, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "ML")

model = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + treatment:I(occasion^2) + age + gender, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "ML")

anova(model_no_gender, model)
```

$H_0:$ the reduced model without gender is better.
The AIC for model_no_gender is less than the AIC for model, and the p-value = 0.2428 > $\alpha = 0.05$, which means that we faili to reject the null hypothesis, and can conclude that the model without gender is better.



```{r}
model_interactions = lme(log_cd4 ~  occasion + I(occasion^2) + 
             treatment:occasion + treatment:I(occasion^2) + 
               age, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "ML")

model_without_interaction = lme(log_cd4 ~  occasion + I(occasion^2) + 
             age, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "ML")

anova(model_interactions, model_without_interaction)
```

$H_0:$ the reduced model without interactions terms is better

Reject the null (p-value <.0001 ), and conclude that the model with interaction term is better compared to the model without without interaction terms
```{r}


model_no_gender = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + treatment:I(occasion^2)
               +  age, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "REML")
summary(model_no_gender)
```

```{r}
model_linear = lme(log_cd4 ~ occasion  + 
             treatment:occasion +  age, data = aids,
             random = ~ occasion | id,
             method = "ML")

model_quadratic = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + treatment:I(occasion^2)
               +  age, data = aids,
             random = ~ occasion + I(occasion^2)| id,
             method = "ML")

summary(model_quadratic)

anova(model_linear, model_quadratic)
```
$H_0:$ the reduced model without quadratic terms are better

Reject the null (p-value <.0001 ), and conclude that the model with quadratic terms better compared linear model


We will first compare a linear model with a linear spline model.  
Linear Spline Model: $E(Y_{ij}|b_i) = \beta_1 + \beta_2occasion_{ij} + \beta_3age_{ij} + \beta_4(occasion_{ij})_+ + \beta_5occasion_{ij}treatment2 + \beta_6occasion_{ij}treatment3 + \beta_7occasion_{ij}treatment4 + b_{1i} + b_{2i}occasion_{ij} + b_{3i}(occasion_{ij})_+$   
```{r}
aids <- aids %>% 
    mutate(knot_term = if_else(occasion > 20, occasion, 0)) %>% 
    relocate(knot_term, .after = occasion)

ctrl <- lmeControl(opt = 'optim')
model_spline = lme(log_cd4 ~ occasion + 
             treatment:occasion + age + knot_term, data = aids,
             random = ~ occasion + knot_term| id,
             method = "ML",
             control = ctrl)
summary(model_spline)
```




```{r}
anova(model_linear, model_spline)
```
$H_0:$ the reduced model (model with no knot term) is better than the spline model (with the knot term).  
Because the AIC of model_no_squared = 12049.15 is less than the AIC of model_spline = 11941.88 (and the p-value < 0.0001 > $\alpha = 0.05$), we reject the null, and conclude that the model with the knot term is better than the reduced model without the knot term. We can see that the linear spline model is better than the linear model.  

### Quadratic Before Knot Term, Linear After Knot Term Spline  
Spline Model: $E(Y_{ij}|b_i) = \beta_1 + \beta_2occasion_{ij} + \beta_3age_{ij} + \beta_4(occasion^2_{ij})_+ + \beta_5occasion_{ij}treatment2 + \beta_6occasion_{ij}treatment3 + \beta_7occasion_{ij}treatment4 + b_{1i} + b_{2i}occasion_{ij} + b_{3i}(occasion^2_{ij})_+$   
```{r}
aids <- aids %>% 
    mutate(knot_term1 = if_else(treatment == 4, occasion^2, 0)) %>% 
    relocate(knot_term1, .after = occasion)

ctrl <- lmeControl(opt = 'optim')
model_spline2 = lme(log_cd4 ~ occasion + 
             treatment:occasion + age + knot_term1, data = aids,
             random = ~ occasion | id,
             method = "ML",
             control = ctrl)
summary(model_spline2)

ctrl <- lmeControl(opt = 'optim')
model_spline1 = lme(log_cd4 ~ occasion + 
             treatment:occasion + age + knot_term1, data = aids,
             random = ~ occasion + knot_term1| id,
             method = "ML",
             control = ctrl)
summary(model_spline1)
```

```{r}
anova(model_spline, model_spline1)
anova(model_linear, model_spline1)
```
Looking at the first ANOVA call, we can see that the model with a quadratic function before the knot term and linear function after the knot term has a lower AIC, which implies that a quadratic spline will fit the data better.  
Second ANOVA call:  
$H_0:$ model_no_squared is better  
Because the p-value <.0001 > $\alpha = 0.05$, we reject the null hypothesis and conclude that the model with a quadratic function before the knot term and linear function after the knot term is better than a linear model.  

```{r}
anova(model_quadratic, model_spline1)
```


When comparing a quadratic model to the model with a quadratic function before the knot term and linear function after the knot term, we can see that the AIC of the quadratic model is lower than the spline model. We can conclude that the quadratic model fits the data better than the spline model.  

### Quadratic Spline Model
Quadratic Spline Model: $E(Y_{ij}|b_i) = \beta_1 + \beta_2occasion_{ij} + \beta_2occasion^2_{ij} + \beta_4age_{ij} + \beta_5(occasion^2_{ij})_+ +  \beta_6occasion_{ij}treatment2 + \beta_7occasion_{ij}treatment3 + \beta_8occasion_{ij}treatment4 + b_{1i} + b_{2i}occasion_{ij} + b_{3i}(occasion^2_{ij})_+$    
```{r}
ctrl <- lmeControl(opt = 'optim')
model_quad_splines = lme(log_cd4 ~ occasion + I(occasion^2) + 
             treatment:occasion + age + knot_term1, data = aids,
             random = ~ occasion + I(occasion^2) + knot_term1| id,
             method = "ML",
             control = ctrl)
summary(model_quad_splines)
```


```{r}
anova(model_quadratic, model_quad_splines)
```
$H_0:$ The reduced model (model_no_gender) is better.  
Because the p-value = 0.4779 > $\alpha = 0.05$, we fail to reject the null. We conclude that the Quadratic model is better than the Quadratic Spline model.  

### We can conclude that it is better to use the quadratic model than both a spline model and a linear model.   

The equation that we have concluded is: has the lowest AIC 

Y = B1 + B2 * occasion + B3 * occasion^2 + B4 * age +
    B5 * occasion:treatment2 + B6 * occasion:treatment3 +
    B7 * occasion:treatment4 + B8 * occasion^2:(treatment2 - 1)+
    B9 * occasion^2:(treatment3 - 1) + B10 * (occasion^2 - 16):treatment4


Note: We did not remove the interaction terms or occasion that are not significant since the other interaction terms or occasion was significant. 
Occasion is significant but occasion^2 is not, and some interaction terms of occasion^2 is siginificant


Notes: 
Compare two models with AIC if they are not nested,
if models are nested use anova
Change the model that we have to what we have in the front, the - 1 are added since they are linear models instead of quadratic that is why they are not significant
and treatment 4 looks like a quadratic but then most of it becomes linear so we can add a knot term. anything before 16 is a quadratic and everything after 16 is linear
